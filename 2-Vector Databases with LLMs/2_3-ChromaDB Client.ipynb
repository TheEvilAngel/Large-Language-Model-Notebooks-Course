{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbba28bc-1f60-41fb-81dc-1b58cda63649",
   "metadata": {
    "id": "fbba28bc-1f60-41fb-81dc-1b58cda63649"
   },
   "source": [
    "<div>\n",
    "    <h1 >Large Language Models Projects</h1>\n",
    "    <h3>Apply and Implement Strategies for Large Language Models</h3>\n",
    "    <h2>2_3-ChromaDB Client</h2>\n",
    "</div>\n",
    "\n",
    "by [Pere Martra](https://www.linkedin.com/in/pere-martra/)\n",
    "\n",
    "This notebook accesses the Chroma server that has been previously created with the notebook 2_2-ChromaDB Server mode.\n",
    "_________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599371fc-8b8b-468a-9347-b393f68ae88a",
   "metadata": {
    "id": "599371fc-8b8b-468a-9347-b393f68ae88a"
   },
   "outputs": [],
   "source": [
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b47aca-86b3-4978-8adb-e0e0152423ef",
   "metadata": {
    "id": "c9b47aca-86b3-4978-8adb-e0e0152423ef"
   },
   "outputs": [],
   "source": [
    "client = chromadb.HttpClient(host='localhost', port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e6316b-1fff-4aa7-a393-10da2f58707e",
   "metadata": {
    "id": "99e6316b-1fff-4aa7-a393-10da2f58707e"
   },
   "outputs": [],
   "source": [
    "collection_local = client.get_collection(name=\"local_news_collection\")\n",
    "results = collection_local.query(query_texts=[\"laptop\"], n_results=10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3432d427-2236-4300-bad7-676b60245830",
   "metadata": {
    "id": "3432d427-2236-4300-bad7-676b60245830",
    "outputId": "1e584ad1-2bce-4045-dc05-12e85669c1d9"
   },
   "outputs": [],
   "source": [
    "print (results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91653be5-7c1e-4f9f-84ed-d76ec573f465",
   "metadata": {
    "id": "91653be5-7c1e-4f9f-84ed-d76ec573f465"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "#model_id = \"databricks/dolly-v2-3b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "lm_model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
